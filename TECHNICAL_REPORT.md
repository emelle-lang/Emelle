# Emmeline

Author: Alan H.

Emmeline is a dialect of the [ML programming language family](
https://en.wikipedia.org/wiki/ML_(programming_language)) that I created both for
my own learning purposes as well as for the purpose of being an educational
programming language that teaches typed functional programming.

Although Emmeline is still a work-in-progress, it is already fairly
sophisticated, currently featuring:

- Hindley-Milner type inference
- Algebraic data types (sum-of-products)
- Pattern matching
- Mutable reference cells
- Records with polymorphic fields
- Typed holes, placeholder expressions that cause the typechecker to print the
  expected type and the current variables in scope

## Implementation Overview

### Lexer and Parser

[syntax/](/syntax)

Emmeline uses the OCamllex lexer generator and the Menhir LR parser generator.
The parser outputs an AST.

### Desugarer

[desugar/](/desugar)

The desugarer translates the AST generated by the parser into a "core," more
minimal language. In the process, the desugarer resolves names, replacing local
variables with unique IDs and file-level names with their fully-qualified
versions.

### Typechecker

[typing/](/typing)

The typechecker accepts the core language as its input and performs
[Hindley-Milner type inference](
https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system) to produce a
type-annotated AST.

The typechecker uses a variation of the Hindley-Milner type inference algorithm
described in http://okmij.org/ftp/ML/generalization.html. When traversing the
program, the typechecker maintains the "level" or "nesting depth" of
let-expressions. Type variables are associated with the let-expression level
that they were created in, and upon let-generalization, only type variables
bounded by the current let-level are universally quantified over. This technique
more efficient than traversing the typing context to check free variables, which
is the version of the Hindley-Milner inference algorithm that is usually
presented.

The typechecker also must account for mutability and avoid universally
quantifying over type variables associated with reference cells. The traditional
way to prevent unsound generalization is through the "value restriction," in
which only definitions that are syntactic "values" can be made polymorphic.
However, the value restriction can impede certain styles of programming, such as
point-free style.

Emmeline does not currently use the value restriction. Instead, type variables
have a "purity" representing whether they have been associated with the
reference cell type, as well as a "lambda level" representing their nested depth
into a lambda expression. Impure type variables that are not contained within a
lambda may not be universally quantified over. When typechecking function
application, the typechecker lowers the lambda levels of type variables in the
function's type.

Additionally, Emmeline supports records with polymorphic fields. To support this
feature, universally quantified type variables ("rigid type variables") are
distinguished from type variables created from constraints ("wobbly type
variables"). Rigid type variables may not be solved for.

### A-Normal Form

[anf/](/anf)

The next step is to convert the typed tree into A-Normal Form (ANF), an
intermediate representation in which each expression has a name. The typed tree
is converted into a linear sequence of let-bound instructions, with each result
stored in a unique "register."

The A-normalization code uses continuation passing style (CPS) so that it can
wrap its parent expression inside a let-expression, in the manner described in
http://matt.might.net/articles/cps-conversion/.
http://matt.might.net/articles/a-normalization/ was also used as a guide.

This phase is also where patterns are compiled into decision trees. A more
detailed explanation is located at doc/pattern-match-compilation.md. The
algorithm used is the one described in the paper "Compiling Pattern Matching to
Good Decision Trees" by Luc Maranget:
http://moscova.inria.fr/~maranget/papers/ml05e-maranget.pdf

### Static-Single Assignment

[ssa/](/ssa)

Strictly speaking, the ANF representation is already in static single assignment
(SSA) form because each atomic instruction is associated with a "register."
Emmeline's "static-single assignment" representation is a lower-level
representation that replaces the pattern match decision tree with a graph of
"basic blocks." Each basic block is a linear sequence of instructions that
either jumps to another basic block or returns from the function.

Compilation to the SSA form is where tail-call optimization occurs.

### Register Allocation

[regalloc/](/regalloc)

In SSA form, each register is only used once, leading to wasted space. Register
allocation computes which memory locations may be reused for different registers
through liveness analysis and coloring. The liveness analyzer performes a
post-order traversal of the basic block control-flow graph, keeping track of
live registers. When it encounters the *definition* of a register
(it walks backwards), it removes the register from the live set. The liveness
analyzer emits a second intermediate representation, SSA2, where each
instruction is annotated with the set of registers where the instruction is the
last instruction to use the register.

The colorer straightforwardly walks the basic block graph and maps SSA registers
to assigned concrete registers, recycling concrete registers when their lifetime
ends. The colorer emits code in the "Asm" representation.

Each concrete register is actually an integer.

### Interpretation

[vm](/vm)

Emmeline has a naive interpreter that executes the Asm representation. Each
physical register is really an index into an array (a "stack frame"). The
trickiest part of evaluation is that a function can either be a "procedure," a
"partial application," or a "foreign" function representing intrinsic
instructions written in OCaml.

## Future Work

### Operators

Although they are merely a special syntax for functions, operators go far to
make a programming language more user-friendly. Users will want to write
arithmetic expressions using familiar symbols such as `+`, or logical
expressions with `<` and `>`. Functional programmers may be familiar with
further operators, such as monadic bind `>>=`. Yet, some criticize languages
such as Haskell for encouraging an overuse of operators, leading to names that
are hard to search up and code that is hard to understand.

Emmeline does not yet support operators. I need to compare the approaches of
different languages: Java has operators for built-in operations but does not
support operator overloading. C++ allows overloading of a fixed set of
operators. OCaml allows custom operators, whose precedences and associativities
are determined by their leading character. Haskell allows custom operators with
custom fixities, which [GHC implements by hackily treating operators as
left-associative, then rearranging the parse tree later](
https://gitlab.haskell.org/ghc/ghc/wikis/commentary/compiler/parser). Agda
allows arbitrary mixfix operators, not just binary operators. In the large
design space around operators, I must compare the tradeoffs of user flexibility,
clarity of programs, and ease of implementation.

### Typeclasses and Module Systems

Generic programming is greatly limited without the ability to express type
variable requirements via interfaces. Currently, Emmeline supports interfaces
via its polymorphic records, which are passed around explicitly.

The ML family is famous for its powerful module language, which traditionally
consists of:

- *Signatures* describing the interface
- *Structures* conforming to the signature
- *Functors*, or functions from modules to modules, allowing code to be
   parameterized over an arbitrary implementation of an interface

Further features of the module system include *abstract*, or existential types,
and sharing of types between modules.

Although the ML module system is powerful, this power comes at a cost: A module
must be created and passed to a functor to accomplish a task as simple as
specifying a hash implementation for a hash table. In contrast, Haskell's
typeclass system trades the flexibility of the ML module system for
convenience. In Haskell, *typeclasses* describe interfaces and *instances*
implement them. Typeclass constraints are specified within a type signature, and
instances are implicitly passed. However, as a consequence, there can only be
one instance of a typeclass for a given combination of types. To provide
multiple instances for the same type, Haskell programmers must use the `newtype`
strong type alias feature.

Module systems is an active research area, with work such as Andreas Rossberg's
1ML, which unifies ML's core and module languages, and OCaml's upcoming *modular
implicits feature* intended to make module usage more convenient. I must further
study the options and tradeoffs of different module system designs.

### Block-Based Syntax

I created Emmeline with the intention of making it a language with a block-based
concrete syntax, in the style of [Scratch](https://scratch.mit.edu) and [Snap!](
https://snap.berkeley.edu). However, Emmeline is complicated by its more
sophisticated grammar. While Scratch and Snap!'s only two syntactic forms are
the statement and expression, Emmeline features syntactic categories such as
patterns and types, not to mention numerous minor, "helper" block categories
such as "Patterns..." for a list of patterns. Text-based languages can have many
nonterminals in their grammar because the user generally does not need to be
aware of them to program in the language. However, in a block-based language,
this formal grammar is exposed to the user. I need to explore ways to simplify
Emmeline's block-based syntax while preserving the features of ML.
