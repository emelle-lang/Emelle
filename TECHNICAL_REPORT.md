# Emmeline

Author: Alan H.

Emmeline is a dialect of the [ML programming language family](
https://en.wikipedia.org/wiki/ML_(programming_language)) that I created for my
own learning purposes as well as for the purpose of being an educational
programming language that teaches typed functional programming.

Although Emmeline is still a work-in-progress, it is already fairly
sophisticated, currently featuring:

- Hindley-Milner type inference
- Algebraic data types (sum-of-products)
- Pattern matching
- Mutable reference cells
- Records with polymorphic fields
- Typed holes, placeholder expressions that cause the typechecker to print the
  expected type and the current variables in scope

## Implementation Overview

### Lexer and parser

Emmeline uses the OCamllex lexer generator and the Menhir LR parser generator.
The parser outputs an AST.

### Desugarer

The desugarer translates the AST generated by the parser into a "core," more
minimal language. In the process, the desugarer resolves names, replacing local
variables with unique IDs and file-level names with their fully-qualified
versions.

### Typechecker

The typechecker accepts the core language as its input and performs
[Hindley-Milner type inference](
https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system) to produce a
type-annotated AST.

The typechecker uses a variation of the Hindley-Milner type inference algorithm
described in http://okmij.org/ftp/ML/generalization.html. When traversing the
program, the typechecker maintains the "level" or "nesting depth" of
let-expressions. Type variables are associated with the let-expression level
that they were created in, and upon let-generalization, only type variables
bounded by the current let-level are universally quantified over. This technique
more efficient than traversing the typing context to check free variables, which
is the version of the Hindley-Milner inference algorithm that is usually
presented.

The typechecker also must account for mutability and avoid universally
quantifying over type variables associated with reference cells. The traditional
way to prevent unsound generalization is through the "value restriction," in
which only definitions that are syntactic "values" can be made polymorphic.
However, the value restriction can impede certain styles of programming, such as
point-free style.

Emmeline does not currently use the value restriction. Instead, type variables
have a "purity" representing whether they have been associated with the
reference cell type, as well as a "lambda level" representing their nested depth
into a lambda expression. Impure type variables that are not contained within a
lambda may not be universally quantified over. When typechecking function
application, the typechecker lowers the lambda levels of type variables in the
function's type.

Additionally, Emmeline supports records with polymorphic fields. To support this
feature, universally quantified type variables ("rigid type variables") are
distinguished from type variables created from constraints ("wobbly type
variables"). Rigid type variables may not be solved for.

### A-Normal Form

The next step is to convert the typed tree into A-Normal Form (ANF), an
intermediate representation in which each expression has a name. The typed tree
is converted into a linear sequence of let-bound instructions, with each result
stored in a unique "register."

The A-normalization code uses continuation passing style (CPS) so that it can
wrap its parent expression inside a let-expression, in the manner described in
http://matt.might.net/articles/cps-conversion/.
http://matt.might.net/articles/a-normalization/ was also used as a guide.

This phase is also where patterns are compiled into decision trees. A more
detailed explanation is located at doc/pattern-match-compilation.md. The
algorithm used is the one described in the paper "Compiling Pattern Matching to
Good Decision Trees" by Luc Maranget:
http://moscova.inria.fr/~maranget/papers/ml05e-maranget.pdf

### Static-Single Assignment

Strictly speaking, the ANF representation is already in static single assignment
(SSA) form because each atomic instruction is associated with a "register."
Emmeline's "static-single assignment" representation is a lower-level
representation that replaces the pattern match decision tree with a graph of
"basic blocks." Each basic block is a linear sequence of instructions that
either jumps to another basic block or returns from the function.

Compilation to the SSA form is where tail-call optimization occurs.

### Register Allocation

In SSA form, each register is only used once, leading to wasted space. Register
allocation computes which memory locations may be reused for different registers
through liveness analysis and coloring. The liveness analyzer performes a
post-order traversal of the basic block control-flow graph, keeping track of
live registers. When it encounters the *definition* of a register
(it walks backwards), it removes the register from the live set. The liveness
analyzer emits a second intermediate representation, SSA2, where each
instruction is annotated with the set of registers where the instruction is the
last instruction to use the register.

The colorer straightforwardly walks the basic block graph and maps SSA registers
to assigned concrete registers, recycling concrete registers when their lifetime
ends. The colorer emits code in the "Asm" representation.

Each concrete register is actually an integer.

### Interpretation

Emmeline has a naive interpreter that executes the Asm representation. Each
physical register is really an index into an array (a "stack frame"). The
trickiest part of evaluation is that a function can either be a "procedure," a
"partial application," or a "foreign" function representing intrinsic
instructions written in OCaml.
